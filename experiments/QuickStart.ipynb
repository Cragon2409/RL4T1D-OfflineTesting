{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98427d31-e441-49ac-9321-5860e5e13eaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PolicyNet Params: 4002\n",
      "ValueNet Params: 3969\n",
      "\n",
      "Experiment Starting...\n",
      "\n",
      "Options =================>\n",
      "{'experiment_folder': 'test64', 'device': 'cpu', 'verbose': True, 'seed': 0, 'debug': True, 'patient_id': 0, 'env_config': '/home/chirath/Documents/G2P2C/RL4T1D/environment/env_config.yaml', 'clinical_config': '/home/chirath/Documents/G2P2C/RL4T1D/agents/std_bb/bb_config.yaml', 'agent': 'ppo', 'rl_config': '/home/chirath/Documents/G2P2C/RL4T1D/agents/configs/ppo_config.yaml', 'debug_config': '/home/chirath/Documents/G2P2C/RL4T1D/experiments/debug_config.yaml', 'env_settings': {'obs_type': 'past_history', 'history_length': 12, 'history_n_features': 2, 'n_actions': 1, 'control_space_type': 'exponential'}, 'n_action': 1, 'n_features': 2, 'feature_history': 12, 'calibration': 12, 'control_space_type': 'exponential', 'sensor': 'GuardianRT', 'glucose_max': 600, 'glucose_min': 39, 'pump': 'Insulet', 'insulin_max': 5, 'insulin_min': 0, 'meal_times_mean': [8, 10.5, 13, 16.5, 20, 22.5], 'time_variance': [60, 30, 60, 30, 60, 30], 'time_lower_bound': [7, 10, 12, 16, 19, 22], 'time_upper_bound': [9, 11, 14, 17, 21, 23], 'meal_prob': [0.95, -1, 0.95, -1, 0.95, -1], 'meal_amount': [45, 30, 85, 30, 80, 30], 'meal_variance': [5, 3, 5, 3, 10, 3], 'val_meal_prob': [1, -1, 1, -1, 1, -1], 'val_meal_amount': [40, 0, 80, 0, 60, 0], 'val_meal_variance': ['1e-8', '1e-8', '1e-8', '1e-8', '1e-8', '1e-8'], 'val_time_variance': ['1e-8', '1e-8', '1e-8', '1e-8', '1e-8', '1e-8'], 't_meal': 20, 'use_meal_announcement': False, 'use_carb_announcement': False, 'use_tod_announcement': False, 'use_bolus': True, 'expert_bolus': False, 'use_cf': False, 'expert_cf': False, 'n_rnn_hidden': 16, 'n_rnn_layers': 1, 'rnn_directions': 1, 'bidirectional': False, 'return_type': 'average', 'gamma': 0.99, 'lambda_': 0.95, 'normalize_reward': True, 'shuffle_rollout': True, 'entropy_coef': 0.001, 'grad_clip': 20, 'eps_clip': 0.1, 'target_kl': 0.01, 'n_step': 256, 'max_epi_length': 2880, 'n_training_workers': 2, 'total_interactions': 4000, 'n_interactions_lr_decay': 2000, 'pi_lr': 0.0003, 'vf_lr': 0.0003, 'batch_size': 64, 'n_pi_epochs': 5, 'n_vf_epochs': 5, 'n_testing_workers': 2, 'max_test_epi_len': 288, 'n_val_trials': 3, 'aux_buffer_max': 1000, 'replay_buffer_size': 1024, 'sample_size': 128, 'experiment_dir': '/home/chirath/Documents/G2P2C/RL4T1D/results/test64'}\n",
      "\n",
      "Device which the program run on: cpu\n",
      "Running Policy Update...\n",
      "The policy loss is: tensor([-0.1427])\n",
      "Running Value Function Update...\n",
      "\n",
      "---------------------------------------------------------\n",
      "Training Progress: 12.80%, Elapsed time: 0.1688 minutes.\n",
      "---------------------------------------------------------\n",
      "Running Policy Update...\n",
      "The policy loss is: tensor([-0.0360])\n",
      "Running Value Function Update...\n",
      "\n",
      "---------------------------------------------------------\n",
      "Training Progress: 25.60%, Elapsed time: 0.1090 minutes.\n",
      "---------------------------------------------------------\n",
      "Running Policy Update...\n",
      "The policy loss is: tensor([-0.0364])\n",
      "Running Value Function Update...\n",
      "\n",
      "---------------------------------------------------------\n",
      "Training Progress: 38.40%, Elapsed time: 0.1881 minutes.\n",
      "---------------------------------------------------------\n",
      "Running Policy Update...\n",
      "The policy loss is: tensor([-0.0294])\n",
      "Running Value Function Update...\n",
      "\n",
      "---------------------------------------------------------\n",
      "Training Progress: 51.20%, Elapsed time: 0.1361 minutes.\n",
      "---------------------------------------------------------\n",
      "Running Policy Update...\n",
      "The policy loss is: tensor([-0.0075])\n",
      "Running Value Function Update...\n",
      "\n",
      "---------------------------------------------------------\n",
      "Training Progress: 64.00%, Elapsed time: 0.2765 minutes.\n",
      "---------------------------------------------------------\n",
      "Running Policy Update...\n",
      "The policy loss is: tensor([-0.0005])\n",
      "Running Value Function Update...\n",
      "\n",
      "---------------------------------------------------------\n",
      "Training Progress: 76.80%, Elapsed time: 0.1603 minutes.\n",
      "---------------------------------------------------------\n",
      "Running Policy Update...\n",
      "The policy loss is: tensor([-4.0778e-05])\n",
      "Running Value Function Update...\n",
      "\n",
      "---------------------------------------------------------\n",
      "Training Progress: 89.60%, Elapsed time: 0.1770 minutes.\n",
      "---------------------------------------------------------\n",
      "Running Policy Update...\n",
      "The policy loss is: tensor([-3.7299e-06])\n",
      "Running Value Function Update...\n",
      "\n",
      "---------------------------------------------------------\n",
      "Training Progress: 100.00%, Elapsed time: 0.2835 minutes.\n",
      "---------------------------------------------------------\n",
      "\n",
      "---------------------------------------------------------\n",
      "===> Starting Validation Trials ....\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/chirath/Documents/G2P2C/RL4T1D/experiments/run_RL_agent.py\", line 55, in <module>\n",
      "    main()\n",
      "  File \"/home/chirath/Documents/G2P2C/RL4T1D/experiments/run_RL_agent.py\", line 51, in main\n",
      "    agent.run()\n",
      "  File \"/home/chirath/Documents/G2P2C/RL4T1D/agents/algorithm/agent.py\", line 84, in run\n",
      "    self.evaluate()\n",
      "  File \"/home/chirath/Documents/G2P2C/RL4T1D/agents/algorithm/agent.py\", line 113, in evaluate\n",
      "    print(calc_stats(res, metric=metric, sim_len=288))\n",
      "  File \"/home/chirath/Documents/G2P2C/RL4T1D/metrics/statistics.py\", line 21, in calc_stats\n",
      "    res = res[target_metrics].describe().loc[metric]\n",
      "  File \"/home/chirath/Documents/G2P2C/lib/python3.10/site-packages/pandas/core/frame.py\", line 4096, in __getitem__\n",
      "    indexer = self.columns._get_indexer_strict(key, \"columns\")[1]\n",
      "  File \"/home/chirath/Documents/G2P2C/lib/python3.10/site-packages/pandas/core/indexes/base.py\", line 6199, in _get_indexer_strict\n",
      "    self._raise_if_missing(keyarr, indexer, axis_name)\n",
      "  File \"/home/chirath/Documents/G2P2C/lib/python3.10/site-packages/pandas/core/indexes/base.py\", line 6251, in _raise_if_missing\n",
      "    raise KeyError(f\"{not_found} not in index\")\n",
      "KeyError: \"['S_hypo', 'S_hyper'] not in index\"\n"
     ]
    }
   ],
   "source": [
    "!python run_RL_agent.py --agent ppo --debug True --experiment_folder test64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1707678-c5ae-48f7-8e2e-37675744b946",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
